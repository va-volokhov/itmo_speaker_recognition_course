{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Лабораторный практикум по курсу «Распознавание диктора», Университет ИТМО, 2021**\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Лабораторная работа №1. Информативные признаки речевых сигналов: извлечение признаков**\n",
    "\n",
    "**Цель работы:** изучение процедуры построения информативных акустических признаков для речевых сигналов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Описание**:\n",
    "В рамках данной лабораторной работы вам предстоит познакомиться с процедурами предобработки речевых сигналов и извлечения информативных признаков. В данной работе вам предлагается научиться извлекать\n",
    "\n",
    "- Mel Filter Banks - кратковременные энергии мелчастотных полос\n",
    "- Mel Frequency Cepstral Coeffitients - Мел частотные кепстральные коэффициенты\n",
    "\n",
    "**Данные**:\n",
    "В качестве данных для проведения исследования предлагается взять часть базы VoxCeleb1: http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Содержание лабораторной работы**\n",
    "\n",
    "1. Выполнить процедуру высокочастотной фильтрации (преэмфазис) речевого сигнала.\n",
    "\n",
    "2. Вычислить акустические признаки разных видов.\n",
    "\n",
    "3. Выполнить локальные центрирование и масштабирование акустических признаков.\n",
    "\n",
    "4. Построить распределение первых 3 компонент полученных аустических признаков для мужских и женских голосов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.realpath('..'))\n",
    "\n",
    "from common import download_dataset, extract_dataset\n",
    "\n",
    "from math import sqrt, pi\n",
    "from scipy.fftpack import dct\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import hist, plot, show, grid, title, xlabel, ylabel, legend, axis, imshow\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from skimage.morphology import opening, closing\n",
    "from torchaudio.transforms import Resample\n",
    "from multiprocessing import Pool\n",
    "import torchaudio\n",
    "\n",
    "from exercises_blank import split_meta_line, preemphasis, framing, power_spectrum \n",
    "from exercises_blank import compute_fbank_filters, compute_fbanks_features, compute_mfcc, mvn_floating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Подготовка данных для анализа**\n",
    "\n",
    "В ходе выполнения лабораторной работы необходимы данные для осуществления процедуры вычисления акустических признаков. Возьмём в качестве этих данных несколько звукозаписей голосов людей мужского и женского пола, сохраненных в формат *wav*, выбранных из корпуса [VoxCeleb1 test set](https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip). Данный корпус содержит 4,874 звукозаписи (частота дискретизации равна 16кГц) 40 дикторов мужского и женского пола, разговаривающих на английском языке.\n",
    "\n",
    "В рамках настоящего пункта требуется выполнить загрузку и распаковку звуковых wav-файлов из корпуса VoxCeleb1 test set.\n",
    "\n",
    "![Рисунок 1](https://analyticsindiamag.com/wp-content/uploads/2020/12/image.png \"VoxCeleb. Крупномасштабная аудиовизуальная база данных человеческой речи.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksum successful vox1_test_wav.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download VoxCeleb1 (test set)\n",
    "with open('../data/lists/datasets.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "download_dataset(lines, user='voxceleb1902', password='nx0bl2v2', save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting of ../data/vox1_test_wav.zip is successful.\n"
     ]
    }
   ],
   "source": [
    "# Extract VoxCeleb1 test set\n",
    "extract_dataset(save_path='../data/voxceleb1_test', fname='../data/vox1_test_wav.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Выполнить процедуру высокочастотной фильтрации (преэмфазис) речевого сигнала**\n",
    "\n",
    "В рамках настоящего пункта предлагается изучить процедуру предобработки речевых сигналов, получившую название преэмфазис. *Преэмфазис (pre-emphasis)* – применение фильтра верхних частот к сигналу до процедуры извлечения признаков для того, чтобы компенсировать тот факт, что обычно вокализованная речь содержит на низких частотах намного больше энергии, чем невокализованная речь на высоких. Обозначим через $x(n)$ обрабатываемый сигнал, а через $y(n)$ результат обработки, тогда процедура преэмфазиса может быть описана с помощью следующего выражения:\n",
    "\n",
    "$$y(n) = x(n) + \\alpha x(n-1).$$\n",
    "\n",
    "Выражение выше представляет собой *линейное разностное уравнение с постоянными коэффициентами* и позволяет описать процедуру работы *нерекурсивного фильтра первого порядка с конечной импульсной характеристикой*. Параметр $\\alpha$ является единственным параметром рассматриваемого фильтра. В случае, когда параметр $\\alpha$ является отрицательным, рассматриваемый фильтр обладает свойствами *фильтра верхних частот*. Для выполнения процедуры преэмфазиса параметр $\\alpha$ обычно выбирается равным величине $-0.97$.\n",
    "\n",
    "В рамках настоящего пункта требуется:\n",
    "\n",
    "1. Загрузить анализируемый речевой сигнал.\n",
    "\n",
    "2. Выполнить процедуру преэмфазиса по отношению к загруженному речевому сигналу.\n",
    "\n",
    "3. Сравнить спектры речевых сигналов до и после процедуры преэмфазиса.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Загрузка файла. Осцилограмма. Спектрограмма.** Для начала давайте загрузим пример речевого сигнала в память компьютера. Сейчас существует достаточно много библиотек для работы со звуковыми файлами (scipy.io.wavfile, soundfile, torchaudio и др). Можно использовать любую из них для работы. Мы остановим свой выбор на torchaudio для удобства и системности дальнейшего изложения. В папке .\\metadata представлен файл meta.txt со списком файлов, которые мы предлагаем использовать в данной лабораторной работе.  он имеет достаточно простой формат строк: \n",
    "\n",
    "> Speaker_ID Gender Path\n",
    "\n",
    "> id10271 m ../data/voxceleb1_test/wav/id10271/1gtz-CUIygI/00006.wav\n",
    "\n",
    ">...\n",
    "\n",
    "Распарсим файл с метаинформацией и загрузим первый по списку файл.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speaker_id' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"/media_new/data/novoselov/ITMO_labs/itmo_speaker_recognition_course/lab1/exercises_blank.py\", line 26, in split_meta_line\n    return speaker_id, gender, file_path\nNameError: name 'speaker_id' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-410d6ac3c73f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0mlist_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mspeaker_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_meta_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_lines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpath_to_wav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'speaker_id' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "path_to_meta = \"./metadata/meta.txt\"\n",
    "p = Pool(1)\n",
    "with open(path_to_meta, 'r') as f:\n",
    "     list_lines = f.readlines()\n",
    "speaker_ids, genders, paths = zip(*p.map(split_meta_line, list_lines[1:]))\n",
    "\n",
    "path_to_wav = paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading of {}\".format(path_to_wav))\n",
    "# Load signal\n",
    "signal, sample_rate = torchaudio.load(path_to_wav)\n",
    "signal = signal.numpy().squeeze(axis=0)\n",
    "signal = signal/np.abs(signal).max()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем осцилограмму и спектрограмму первых 3.5 секунд речевого сигнала:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = signal[0:int(3.5 * sample_rate)]  # Keep the first 3.5 seconds\n",
    "\n",
    "plt.figure(1, figsize=(15,5))\n",
    "plot_a = plt.subplot(211)\n",
    "plt.subplots_adjust(wspace=0, hspace=0.5)\n",
    "\n",
    "plot_a.plot(signal)\n",
    "plot_a.set_xlabel('sample rate * time')\n",
    "plot_a.set_ylabel('Amplitude')\n",
    "plot_a.grid()\n",
    "\n",
    "plot_b = plt.subplot(212)\n",
    "plot_b.specgram(signal, NFFT=1024, Fs=sample_rate, noverlap=900)\n",
    "plot_b.set_xlabel('Time')\n",
    "plot_b.set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Процедура преэмфазиса по отношению к загруженному речевому сигналу. Сравнение спектров**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emphasized_signal = preemphasis(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем спектрограммы обработанного и сырого сигналов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plot_a = plt.subplot(211)\n",
    "plt.subplots_adjust(wspace=0, hspace=1)\n",
    "\n",
    "plot_a.specgram(signal, NFFT=1024, Fs=sample_rate, noverlap=900)\n",
    "plot_a.set_xlabel('Time')\n",
    "plot_a.set_ylabel('Frequency')\n",
    "plot_a.title.set_text('Original')\n",
    "\n",
    "plot_b = plt.subplot(212)\n",
    "plot_b.specgram(emphasized_signal, NFFT=1024, Fs=sample_rate, noverlap=900)\n",
    "plot_b.set_xlabel('Time')\n",
    "plot_b.set_ylabel('Frequency')\n",
    "plot_b.title.set_text('Emphasized')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Вычислить акустические признаки разных видов**\n",
    "\n",
    "Работа современных систем голосовой биометрии, как правило, является основанной на использовании акустических признаков. *Акустические признаки* являются формой представления речевого сигнала в частотно-временной области. Возможным примером акустических признаков является *спектрограмма* сигнала, которую можно вычислить с использованием *оконного преобразования Фурье*. Однако, использование спектрограммы на практике не является выгодным, в частности, из-за её большого разрешения по частоте. Поэтому возможным вариантом для построения акустических признаков могут являться: *логарифмов энергий на выходе мел-банка фильтров*, *мел-частотные кепстральные коэффициенты размерности* и т.п.\n",
    "\n",
    "В рамках настоящего пункта предлагается вычислить логарифмы энергий на выходе мел-банка фильтров размерности 40 и мел-частотные кепстральные коэффициенты размерности 23. Для выполнения задания выше можно воспользоваться следующим алгоритмом:\n",
    "\n",
    "1. Представить обрабатываемый речевой сигнал в виде набора фреймов с использованием *окна Хэмминга* (размер окна можно выбрать равным 25мс, а шаг окна – 10мс).\n",
    "\n",
    "2. Вычислить одномерный спектр Фурье по отношению к каждому из фреймов.\n",
    "\n",
    "3. Рассчитать мел-банк фильтров.\n",
    "\n",
    "4. Перемножить квадрат *амплитудно-частотной характеристики (АЧХ)* каждого фильтра со спектром мощности каждого из фреймов ⁡и просуммировать коэффициенты получившихся спектров, рассчитав энергии внутри соответствующих полос банка фильтров.\n",
    "\n",
    "5. Вычислить логарифм от значений энергий на предыдущем шаге. *На этом шаге формируется первый тип акустических признаков.*\n",
    "\n",
    "6. Вычислить дискретное косинусное преобразование от логарифмов коэффициентов. *На этом этапе формируются мел-частотные кепстральные коэффициенты.*\n",
    "\n",
    "Подробное описание процедуры вычисления акустических признаков можно найти по следующей [ссылке](http://practicalcryptography.com/miscellaneous/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1. Представление  речевого сигнала в виде набора фреймов с использованием окна Хэмминга (размер окна можно выбрать равным 25мс, а шаг окна – 10мс)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = framing(emphasized_signal)\n",
    "print(\"Features shape is: {}\".format(frames.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Произведем расчет спектра мощности для каждого фрэйма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow_frames = power_spectrum(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Рассчет мел-банк фильтров**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbank = compute_fbank_filters(nfilt=40, sample_rate=16000, NFFT=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем АЧХ полученных фильтров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plot_a = plt.subplot()\n",
    "plt.subplots_adjust(wspace=0, hspace=1)\n",
    "\n",
    "nfilt = fbank.shape[0]\n",
    "for k in range(nfilt):\n",
    "    plot_a.plot(fbank[k,:])\n",
    "    \n",
    "plot_a.set_xlabel('Frequency bins')\n",
    "plot_a.set_ylabel('Amplitude')\n",
    "plot_a.title.set_text('Fbank')\n",
    "plot_a.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4. Рассчет энергии сигнала внутри соответствующих полос банка фильтров**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_banks_features = compute_fbanks_features(pow_frames, fbank)\n",
    "\n",
    "plt.figure(3, figsize=(15,5))\n",
    "plt.subplots_adjust(wspace=0, hspace=1)\n",
    "\n",
    "plot_a = plt.subplot(211)\n",
    "plot_a.plot(signal)\n",
    "plot_a.set_xlabel('sample rate * time')\n",
    "plot_a.set_ylabel('Amplitude')\n",
    "plot_a.grid()\n",
    "\n",
    "plot_b = plt.subplot(212)\n",
    "plot_b.imshow(filter_banks_features.T, origin='lower')\n",
    "\n",
    "plot_b.set_xlabel('Time bins')\n",
    "plot_b.set_ylabel('Frequency band bins')\n",
    "plot_b.title.set_text('Fbank Energies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6. Вычисление дискретного косинусного преобразования от логарифмов коэффициентов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = compute_mfcc(filter_banks_features, num_ceps=20)\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.subplots_adjust(wspace=0, hspace=1)\n",
    "plot_a = plt.subplot(211)\n",
    "plot_a.plot(signal)\n",
    "plot_a.set_xlabel('sample rate * time')\n",
    "plot_a.set_ylabel('Amplitude')\n",
    "plot_a.grid()\n",
    "\n",
    "plot_b = plt.subplot(212)\n",
    "im = plot_b.imshow(mfcc.T, origin='lower')\n",
    "\n",
    "divider = make_axes_locatable(plot_b)\n",
    "cax = divider.append_axes(\"right\", size=\"1%\", pad=0.05)\n",
    "plt.colorbar(im, cax=cax)\n",
    "\n",
    "plot_b.set_xlabel('Time bins')\n",
    "plot_b.set_ylabel('Coeffitients bins')\n",
    "plot_b.title.set_text('MFCCs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Выполнить локальные центрирование и масштабирование акустических признаков**\n",
    "\n",
    "Канал связи может вносит некоторое смещение в захваченный сигнал (микрофон может не иметь равномерную АЧХ, изменения в усилении\n",
    "сигнал приводят к вычислению различных акустических признаков даже для одного и того же куска речи). Канальный эффект можно промоделировать с использованием линейного инвариантного к сдвигу фильтра (ЛИС-фильтра) и учесть при корректировке значений акустических признаков. Данная процедура получила название *процедуры нормализации*.\n",
    "\n",
    "Идея выполнения процедуры нормализации состоит в вычислении среднего вектора наблюдаемых акустических признаков и центрирования всех векторов акустических признаков произнесения на это среднее. Процедура нормализации может быть выполнена несколькими различными способами:\n",
    "\n",
    "1. Локально по произнесению (вычисляем среднее в некоторой окрестности (обычно 300 фреймов) каждого фрейма стека признаков и нормализуем на них этот фрейм).\n",
    "\n",
    "2. Глобально по произнесению (вычисляем среднее один раз по всей сессии и нормализуем на них все фреймы сессии).\n",
    "\n",
    "3. глобально по базе данных (вычисляем среднее общее для всех сессий в тренировочной базе данных и нормализуем на них все фреймы всех сессий).\n",
    "\n",
    "Иногда процедура нормализации сопровождается процедурой *масштабирования акустических признаков*. Процедура масштабирования признаков может быть выполнена теми же способами, что и процедура нормализации. Отличие процедуры масштабирования признаков от процедуры нормализации состоит в том, что при её выполнении необходимо вычислить не средний вектор акустических признаков, а вектор среднеквадратического отклонения, на который необходимо поэлементно разделить каждый нормализованный вектор акустических признаков некоторого произнесения.\n",
    "\n",
    "Предполагая, что набор акустических признаков до выполнения процедур нормализации и масштабирования был задан функцией $|X(k, m)|$, а после выполнения данных процедур – $|X_{norm}(k, m)|$, запишем следующее выражение:\n",
    "\n",
    "$$|X_{norm}(k, m)| = \\frac{|X_{norm}(k, m)| - m_X}{\\sigma_X},$$\n",
    "\n",
    "где $m_X$ – это средний вектор акустических признаков, $\\sigma_X$ – это вектор среднеквадратического отклонения акустических признаков, $k$ и $m$ имеют суть частоты и времени (k – номер спектральной составляющей, а m – это номер фрейма).\n",
    "\n",
    "В рамках настоящего пункта требуется выполнить процедуры нормализации и масштабирования по отношению к логарифмам энергий на выходе банка фильтров и мел-частотным кепстральным коэффициентам, вычисленным для некоторой звукозаписи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_cmvn = mvn_floating(mfcc, 150, 150)\n",
    "filter_banks_features_mvn = mvn_floating(filter_banks_features, 150, 150)\n",
    "\n",
    "fig = plt.figure(3, figsize=(15,5))\n",
    "plt.subplots_adjust(wspace=0, hspace=1)\n",
    "\n",
    "plot_b = plt.subplot(211)\n",
    "im_b = plot_b.imshow(filter_banks_features_mvn.T, origin='lower')\n",
    "divider = make_axes_locatable(plot_b)\n",
    "cax = divider.append_axes(\"right\", size=\"1%\", pad=0.05)\n",
    "plt.colorbar(im_b, cax=cax)\n",
    "plot_b.set_xlabel('Time bins')\n",
    "plot_b.set_ylabel('Coeffitients bins')\n",
    "plot_b.title.set_text('Normaized FBanks')\n",
    "\n",
    "plot_с = plt.subplot(212)\n",
    "im_c = plot_с.imshow(mfcc_cmvn.T, origin='lower')\n",
    "divider = make_axes_locatable(plot_с)\n",
    "cax = divider.append_axes(\"right\", size=\"1%\", pad=0.05)\n",
    "plt.colorbar(im_c, cax=cax)\n",
    "plot_с.set_xlabel('Time bins')\n",
    "plot_с.set_ylabel('Coeffitients bins')\n",
    "plot_с.title.set_text('Normaized MFCCs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Построить распределение первых 3 компонент полученных аустических признаков для мужских и женских голосов**\n",
    "\n",
    "Для того, чтобы грубо проверить правильность расчёта акустических признаков выше, построим гистограммы распределения первых 3 компонент логарифмов энергий на выходе банка фильтров и мел-частотных кепстральных коэффициентов по некоторой базе данных. Рассмотрим в качестве этой базы звукозаписи мужских и женских голосов дикторов, список которых представлен в ./metadata/meta.txt. В указанном файле перечислен список из 20 звукозаписей (10 для дикторов женского пола и 10 для дикторов мужского пола) из базы VoxCeleb1 test set с указанием пути до них, идентификационного номера диктора и пола диктора.\n",
    "\n",
    "Используя звукозаписи, список которых перечислен в ./metadata/meta.txt, выполнить построение гистограмм распределения первых 3 компонент логарифмов энергий на выходе банка фильтров и мел-частотных кепстральных коэффициентов отдельно для базы мужских и женских голосов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbank = compute_fbank_filters(nfilt=40, sample_rate=16000, NFFT=512)\n",
    "\n",
    "def compute_feats(signal):\n",
    "    \n",
    "    emphasized_signal = preemphasis(signal)\n",
    "    frames = framing(emphasized_signal)\n",
    "    pow_frames = power_spectrum(frames)\n",
    "    filter_banks_features = compute_fbanks_features(pow_frames, fbank)\n",
    "    mfcc = compute_mfcc(filter_banks_features, num_ceps=20)\n",
    "#     mfcc = mvn_floating(mfcc, 150, 150)\n",
    "#     filter_banks_features = mvn_floating(filter_banks_features, 150, 150)\n",
    "    \n",
    "    return filter_banks_features, mfcc\n",
    "\n",
    "male_fb_features = []\n",
    "female_fb_features = []\n",
    "male_mfcc_features = []\n",
    "female_mfcc_features = []\n",
    "\n",
    "for (path_to_wav, gender) in zip(paths, genders):\n",
    "    # Load signal\n",
    "    signal, sample_rate = torchaudio.load(path_to_wav)\n",
    "    signal = signal.numpy().squeeze(axis=0)\n",
    "    signal = signal/np.abs(signal).max()\n",
    "    \n",
    "    ## processing\n",
    "    filter_banks_mvn, mfcc_cmvn = compute_feats(signal)\n",
    "    if gender == 'm':\n",
    "        male_fb_features.append(filter_banks_mvn)\n",
    "        male_mfcc_features.append(mfcc_cmvn)\n",
    "    else:\n",
    "        female_fb_features.append(filter_banks_mvn) \n",
    "        female_mfcc_features.append(mfcc_cmvn)\n",
    "    ##\n",
    "    \n",
    "male_fb_features = np.concatenate(male_fb_features)\n",
    "print(male_fb_features.shape)\n",
    "\n",
    "female_fb_features = np.concatenate(female_fb_features)\n",
    "print(female_fb_features.shape)\n",
    "\n",
    "male_mfcc_features = np.concatenate(male_mfcc_features)\n",
    "print(male_mfcc_features.shape)\n",
    "\n",
    "female_mfcc_features = np.concatenate(female_mfcc_features)\n",
    "print(female_mfcc_features.shape)\n",
    "\n",
    "\n",
    "comp_number = 4\n",
    "coeff1_male = male_fb_features[:,comp_number]\n",
    "coeff1_female = female_fb_features[:,comp_number]\n",
    "\n",
    "min_coeff1 = min(coeff1_male.min(),coeff1_female.min())\n",
    "max_coeff1 = min(coeff1_male.max(),coeff1_female.max())\n",
    "\n",
    "hist(coeff1_male, int(sqrt(len(coeff1_male))), histtype='step', color='green', range=(min_coeff1, max_coeff1), density=1)\n",
    "hist(coeff1_female, int(sqrt(len(coeff1_female))), histtype='step', color='red',   range=(min_coeff1, max_coeff1), density=1)\n",
    "xlabel('$FB1$'); ylabel('$PDFs$'); title('VoxCeleb1-O (cleaned), histograms'); grid(); show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Контрольные вопросы**\n",
    "\n",
    "1. Какие способы представления сигналов существуют?\n",
    "\n",
    "2. Что такое спектр Фурье (амплитудный и фазовый)?\n",
    "\n",
    "3. Что такое оконное преобразование Фурье?\n",
    "\n",
    "4. Что такое спектрограмма?\n",
    "\n",
    "5. Как выполнить процедуру преэмфазиса?\n",
    "\n",
    "6. Описать процедуру вычисления акустических признаков.\n",
    "\n",
    "7. Для каких целей выполняются процедуры нормализации и масштабирования акустических признаков?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Список литературы**\n",
    "\n",
    "1. Hayes M.H. Schaum’s outlines of digital signal processing. McGraw-Hill, 2011 ([ссылка](http://index-of.co.uk/DSP-Collection/Hayes,%20M.H.,%20Schaum's%20Outline%20of%20Digital%20Signal%20Processing,%20McGraw-Hill,%201999.pdf)).\n",
    "\n",
    "2. Beigi H. Fundamentals of speaker recogniton. Springer, 2011 ([ссылка](https://www.springer.com/gp/book/9780387775913))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
